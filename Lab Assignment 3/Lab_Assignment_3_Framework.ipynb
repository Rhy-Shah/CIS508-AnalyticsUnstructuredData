{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RHAC5sxYxzJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgReYw-olAfx"
      },
      "source": [
        "# **Lab Assignment 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP47Y8TGok6G"
      },
      "source": [
        "## Assignment Metadata\n",
        "\n",
        "Write down your information here including author name, your ASU ID, and the file creation date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g29lQSovToT"
      },
      "source": [
        "### You must use this provided framework as-is and follow all the directions stated in there:\n",
        "*\tDo NOT edit or otherwise change any part of this framework except the cells you have been instructed to code.\n",
        "*\tYou must NOT import any additional python packages to complete this assignment\n",
        "\n",
        "### Note that, points will be deducted for notebooks that:\n",
        "*\tdo not follow the saving guidelines/naming convention\n",
        "*\tdo not compile/run\n",
        "*\tdo not follow all the instructions in the framework\n",
        "*\tare incomplete\n",
        "*\thave logical errors\n",
        "*\tdonâ€™t result in the expected output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swN48cdllBhr"
      },
      "source": [
        "## Exercise 1 (Gradient Descent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUeyjzDqbAiK"
      },
      "source": [
        "* In this assignment, you will first use the gradient descent algorithm to find the optimal value of w1 which minimizes the given lost function (Steps 1.1 through 1.5). You will earn 5% points for each step and 25% in total.\n",
        "\n",
        "* You will run the algorithm for 4 different scenarios, each with a different value of alpha. For each scenario, you will log the responses in the last cell of this notebook (Step 1.6). You will earn 5% points for each scenario and 20%  in total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzBsIAYXbAiM"
      },
      "outputs": [],
      "source": [
        "# DO NOT edit this cell!\n",
        "# You must use this lost function and its derivative as-is\n",
        "\n",
        "# lost at particular value of w1\n",
        "def calc_J(w1):\n",
        "    return (5.5*w1*w1 - 1463*w1 + 100812.5)\n",
        "\n",
        "# slope at particular value of w1\n",
        "def calc_dJ_dw1(w1):\n",
        "    return (11*w1 - 1463)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMUYheJcbAiO"
      },
      "outputs": [],
      "source": [
        "# First, complete coding Steps 1.1 through 1.5 in this cell.\n",
        "# Then, run this cell for each of the four scenarios below,\n",
        "# and log your responses for each scenario run in the next cell (Step 1.6)\n",
        "alpha=0.08; niterations = 25; w1 = 60 # Scenario 1\n",
        "# alpha=0.05; niterations = 25; w1 = 60 # Scenario 2\n",
        "# alpha=0.15; niterations = 25; w1 = 60 # Scenario 3\n",
        "# alpha=0.19; niterations = 25; w1 = 60 # Scenario 4\n",
        "\n",
        "# Step 1.1: calculate current value of J at current value of w1\n",
        "# your code here\n",
        "\n",
        "\n",
        "# Step 1.2: print w1 (rounded to 2 decimal places) and J (rounded to 2 decimal places)\n",
        "# your code here\n",
        "\n",
        "\n",
        "# run gradient descent for niterations\n",
        "for i in range(niterations):\n",
        "\n",
        "    # Step 1.3: calculate new value of w1\n",
        "    # your code here\n",
        "\n",
        "\n",
        "    # Step 1.4: calculate new value of J at new w1\n",
        "    # your code here\n",
        "\n",
        "\n",
        "    # Step 1.5: print w1 (rounded to 2 decimal places) and J (rounded to 2 decimal places)\n",
        "    # your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8XQpscNbAiO"
      },
      "outputs": [],
      "source": [
        "# Step 1.6: for each Scenario, answer the following questions based on the output (20%)\n",
        "# type your answers below for each question\n",
        "#\n",
        "### Scenario 1 (alpha=0.08; niterations = 25; w1 = 60):\n",
        "# did the cost steadily decrease as training progressed?\n",
        "# did the gradient descent algorithm converge?\n",
        "# what is the minimum lost?\n",
        "# what is the optimal value of w1?\n",
        "# in how many iterations did gradient descent converge (at which row # in the output)?\n",
        "#\n",
        "### Scenario 2 (alpha=0.05; niterations = 25; w1 = 60):\n",
        "# did the cost steadily decrease as training progressed?\n",
        "# did the gradient descent algorithm converge?\n",
        "# what is the minimum lost?\n",
        "# what is the optimal value of w1?\n",
        "# in how many iterations did gradient descent converge (at which row # in the output)?\n",
        "#\n",
        "### Scenario 3 (alpha=0.15; niterations = 25; w1 = 60):\n",
        "# did the cost steadily decrease as training progressed?\n",
        "# did the gradient descent algorithm converge?\n",
        "# what is the minimum lost?\n",
        "# what is the optimal value of w1?\n",
        "# in how many iterations did gradient descent converge (at which row # in the output)?\n",
        "#\n",
        "### Scenario 4 (alpha=0.19; niterations = 25; w1 = 60):\n",
        "# did the cost steadily decrease as training progressed?\n",
        "# did the gradient descent algorithm converge?\n",
        "# what is the minimum lost?\n",
        "# what is the optimal value of w1?\n",
        "# in how many iterations did gradient descent converge (at which row # in the output)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16sJ8vw2lFf7"
      },
      "source": [
        "## Exercise 2 (Single Neuron)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgCVy2aUgUMj"
      },
      "source": [
        "### Then, you will apply deep learning thinking in modeling a single neuron to build logistic regression and linear regression models. You need also to Apply the idea of vectorization to efficiently train your models and implement gradient descent from scratch (using multiple iterations of the forward pass, backward propagation, and gradient descent steps) to minimize the loss functions and determine model parameters.\n",
        "\n",
        "### Specifically, there are two parts in this assignment:\n",
        "* In the first part, you will train a logistic regression model using the breast cancer data set. This part has 7 parts (2.1.1 through 2.1.7) that you will need to code (see framework for more details).\n",
        "* In the second part, you will train a linear regression model using the diabetes data set. This part has 7 parts (2.2.1 through 2.2.7) that you will need to code (see framework for more details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM2vgB55gcPu"
      },
      "outputs": [],
      "source": [
        "### DO NOT EDIT THIS CELL!!!\n",
        "### DO NOT IMPORT ANY ADDITIONAL PACKAGES\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG7_IZIVhzGZ"
      },
      "source": [
        "### PART 1\n",
        "\n",
        "* Logistic Regression for Binary Classification using the breast cancer dataset\n",
        "\n",
        "* We aren't going to preprocess data or split into train/test since the focus here is purely on the gradient descent algorithm\n",
        "\n",
        "* NOTE: YOU WILL ONLY EDIT ONE CELL FOR PART-1 (SEE BELOW) (5% for each step, but 25% in total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P8AGX0PhoCA"
      },
      "outputs": [],
      "source": [
        "### DO NOT EDIT THIS CELL!!!\n",
        "\n",
        "# read the breast cancer dataset from sklearn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "dataset = load_breast_cancer()\n",
        "\n",
        "# feature matrix X: mxn\n",
        "X = dataset.data\n",
        "print (\"X: \", X.shape)\n",
        "\n",
        "# target vector y: mx1\n",
        "y = dataset.target\n",
        "y = y.reshape(y.shape[0], 1)\n",
        "print (\"y: \", y.shape)\n",
        "\n",
        "# number of samples: m\n",
        "m = X.shape[0]\n",
        "print (\"m: \", m)\n",
        "\n",
        "# number of features: n\n",
        "n = X.shape[1]\n",
        "features = dataset.feature_names\n",
        "print (\"n: \",n)\n",
        "print (\"features: \", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H6Zy2Oxh5Gc"
      },
      "outputs": [],
      "source": [
        "### THIS IS THE ONLY CELL YOU MUST EDIT FOR PART-1\n",
        "\n",
        "# train the logistic regression model\n",
        "\n",
        "# initializae hyperparameters nepochs and alpha\n",
        "nepochs = 5000\n",
        "alpha = 2.5e-6\n",
        "\n",
        "# initialize model parameters w and b\n",
        "w = np.zeros((n,1))\n",
        "b = 0\n",
        "\n",
        "# this list will collect loss for each iteration\n",
        "costs = []\n",
        "\n",
        "# repeat for nepochs\n",
        "### (2.1.1) YOUR CODE HERE\n",
        "# for....\n",
        "\n",
        "    # forward pass (calculate current loss)\n",
        "    ### (2.1.2) YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # backward propagation (calculate current gradient)\n",
        "    ### (2.1.3) YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # gradient descent (update parameters)\n",
        "    ### (2.1.4) YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # append loss to costs: This step is done to keep track of how the loss changes over the course of training\n",
        "    # Costs: It is likely initialized as an empty list before the training loop. It is used to store the loss values at each iteration.\n",
        "    ### (2.1.5) YOUR CODE HERE\n",
        "\n",
        "\n",
        "# use final parameters w and b to calcualte accuracy for training data X, y\n",
        "#  - do a single forward pass for X: calcualte z, yhat\n",
        "#  - set yhat to round(yhat) (that is, thresholding binary classification prediction at 0.5)\n",
        "#  - calcualte metrics.accuracy_score(y, yhat) and print it\n",
        "### (2.1.6) YOUR CODE HERE\n",
        "\n",
        "\n",
        "# use plt.plot(costs)  to plot  costs against iterations and show the plot\n",
        "### (2.1.7) YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C3LactGiB2c"
      },
      "outputs": [],
      "source": [
        "### DO NOT EDIT THIS CELL!!!\n",
        "\n",
        "# compare results from your implementation to that of SGDClassifier\n",
        "# your accuracy score above should match (or be very close to) what you get from here\n",
        "\n",
        "nepochs = 5000\n",
        "alpha = 2.5e-6\n",
        "y = y.reshape(y.shape[0],) # to avoid warning\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "clf = SGDClassifier(loss='log_loss', # loss function for logistic regression\n",
        "                    penalty=None, alpha=0, # no regularization\n",
        "                    max_iter=nepochs, tol=None, # stop based only on nepochs\n",
        "                    shuffle=False, random_state=0, # don't shuffle, use random state to replicate results\n",
        "                    learning_rate='constant', eta0=alpha) # constant learning rate of alpha\n",
        "clf.fit(X,y)\n",
        "print (round(clf.score(X,y),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYG7VIaliH4K"
      },
      "source": [
        "### Part 2\n",
        "\n",
        "* Linear Regression using the diabetes dataset\n",
        "\n",
        "* We aren't going to preprocess data or split  into train/test since the focus here is purely on the gradient descent algorithm\n",
        "\n",
        "* NOTE: YOU WILL ONLY EDIT ONE CELL FOR PART-2 (SEE BELOW) (5% for each step, but 25% in total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lli8Djc-Y1Pm"
      },
      "outputs": [],
      "source": [
        "### DO NOT EDIT THIS CELL!!!\n",
        "\n",
        "# read the diabetes dataset from sklearn\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "dataset = load_diabetes()\n",
        "\n",
        "# feature matrix X: mxn\n",
        "X = dataset.data\n",
        "print (\"X: \", X.shape)\n",
        "\n",
        "# target vector y: mx1\n",
        "y = dataset.target\n",
        "y = y.reshape(y.shape[0], 1)\n",
        "print (\"y: \", y.shape)\n",
        "\n",
        "# number of samples: m\n",
        "m = X.shape[0]\n",
        "print (\"m: \", m)\n",
        "\n",
        "# number of features: n\n",
        "n = X.shape[1]\n",
        "features = dataset.feature_names\n",
        "print (\"n: \",n)\n",
        "print (\"features: \", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBPCJeyWiOHC"
      },
      "outputs": [],
      "source": [
        "### THIS IS THE ONLY CELL YOU MUST EDIT FOR PART-2\n",
        "\n",
        "# train the linear regression model\n",
        "\n",
        "# initialize hyperparameters nepochs and alpha\n",
        "nepochs = 5000\n",
        "alpha = 0.5\n",
        "\n",
        "# initialize model paramters w and b\n",
        "w = np.zeros((n,1))\n",
        "b = 0\n",
        "\n",
        "# this list will collect loss for each iteration\n",
        "costs = []\n",
        "\n",
        "# repeat for nepochs\n",
        "### (2.2.1) YOUR CODE HERE\n",
        "# for ...\n",
        "\n",
        "    # forward pass (calculate current loss)\n",
        "    ### (2.2.2) YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # backward propagation (calculate current gradient)\n",
        "    ### (2.2.3) YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # gradient descent (update parameters)\n",
        "    ### (2.2.4) YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # append loss to costs: This step is done to keep track of how the loss changes over the course of training\n",
        "    # Costs: It is likely initialized as an empty list before the training loop. It is used to store the loss values at each iteration.\n",
        "    ### (2.2.5) YOUR COD HERE\n",
        "\n",
        "\n",
        "# use final parameters w and b to calcualte mse and r2 for training data X, y\n",
        "#  - do a single forward pass for X: calcualte z, yhat\n",
        "#  - calculate metrics.mean_squared_error(y, yhat) and print it\n",
        "#  - calculate metrics.r2_score(y, yhat) and print it\n",
        "### (2.2.6) YOUR CODE HERE\n",
        "\n",
        "\n",
        "# use plt.plot(costs) to plot costs against iterations and show the plot\n",
        "### (2.2.7) YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0fI3RBEiTGx"
      },
      "outputs": [],
      "source": [
        "### DO NOT EDIT THIS CELL!!!\n",
        "\n",
        "# compare results from our implementation to that of SGDRegressor\n",
        "# your mse and r2 score above should match (or be very close to) what you get from here\n",
        "\n",
        "nepochs = 5000\n",
        "alpha = 2.5e-6\n",
        "y = y.reshape(y.shape[0],) # to avoid warning\n",
        "\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "clf = SGDRegressor(loss='squared_error', # mse for linear regression\n",
        "                    penalty=None, alpha=0, # no regularization\n",
        "                    max_iter=nepochs, tol=None, # stop based on only on nepochs\n",
        "                    shuffle=False, random_state=0, # don't shuffle, use random state to replicate results\n",
        "                    learning_rate='constant', eta0=alpha) # constant learning rate of alpha\n",
        "clf.fit(X,y)\n",
        "yhat = clf.predict(X)\n",
        "print (round(metrics.mean_squared_error(y, yhat),2))\n",
        "print (round(clf.score(X,y),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxneXnBSqLgk"
      },
      "source": [
        "## Acknowledgment\n",
        "\n",
        "Acknowledge here if you have used any GenAI tools in this assignment and anyone you have worked together with on this assignment. (5%)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
