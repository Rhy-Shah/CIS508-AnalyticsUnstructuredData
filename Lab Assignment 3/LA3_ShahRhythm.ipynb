{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RHAC5sxYxzJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgReYw-olAfx"
      },
      "source": [
        "# **Lab Assignment 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP47Y8TGok6G"
      },
      "source": [
        "## Assignment Metadata\n",
        "\n",
        "* Rhythm Shah\n",
        "* 1233960561\n",
        "* 02/16/25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g29lQSovToT"
      },
      "source": [
        "### You must use this provided framework as-is and follow all the directions stated in there:\n",
        "*\tDo NOT edit or otherwise change any part of this framework except the cells you have been instructed to code.\n",
        "*\tYou must NOT import any additional python packages to complete this assignment\n",
        "\n",
        "### Note that, points will be deducted for notebooks that:\n",
        "*\tdo not follow the saving guidelines/naming convention\n",
        "*\tdo not compile/run\n",
        "*\tdo not follow all the instructions in the framework\n",
        "*\tare incomplete\n",
        "*\thave logical errors\n",
        "*\tdonâ€™t result in the expected output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swN48cdllBhr"
      },
      "source": [
        "## Exercise 1 (Gradient Descent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUeyjzDqbAiK"
      },
      "source": [
        "* In this assignment, you will first use the gradient descent algorithm to find the optimal value of w1 which minimizes the given lost function (Steps 1.1 through 1.5). You will earn 5% points for each step and 25% in total.\n",
        "\n",
        "* You will run the algorithm for 4 different scenarios, each with a different value of alpha. For each scenario, you will log the responses in the last cell of this notebook (Step 1.6). You will earn 5% points for each scenario and 20%  in total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lzBsIAYXbAiM"
      },
      "outputs": [],
      "source": [
        "# DO NOT edit this cell!\n",
        "# You must use this lost function and its derivative as-is\n",
        "\n",
        "# lost at particular value of w1\n",
        "def calc_J(w1):\n",
        "    return (5.5*w1*w1 - 1463*w1 + 100812.5)\n",
        "\n",
        "# slope at particular value of w1\n",
        "def calc_dJ_dw1(w1):\n",
        "    return (11*w1 - 1463)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tMUYheJcbAiO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32832.5\n",
            "Iteration 0 :  w1: 212.57   J: 38345.62\n",
            "Iteration 1 :  w1: 46.27   J: 44895.75\n",
            "Iteration 2 :  w1: 227.54   J: 52677.97\n",
            "Iteration 3 :  w1: 29.95   J: 61924.01\n",
            "Iteration 4 :  w1: 245.32   J: 72909.25\n",
            "Iteration 5 :  w1: 10.57   J: 85960.8\n",
            "Iteration 6 :  w1: 266.45   J: 101467.35\n",
            "Iteration 7 :  w1: -12.46   J: 119890.68\n",
            "Iteration 8 :  w1: 291.55   J: 141779.44\n",
            "Iteration 9 :  w1: -39.82   J: 167785.48\n",
            "Iteration 10 :  w1: 321.37   J: 198683.25\n",
            "Iteration 11 :  w1: -72.32   J: 235392.89\n",
            "Iteration 12 :  w1: 356.8   J: 279007.62\n",
            "Iteration 13 :  w1: -110.95   J: 330826.28\n",
            "Iteration 14 :  w1: 398.9   J: 392392.02\n",
            "Iteration 15 :  w1: -156.83   J: 465538.29\n",
            "Iteration 16 :  w1: 448.92   J: 552443.36\n",
            "Iteration 17 :  w1: -211.35   J: 655695.28\n",
            "Iteration 18 :  w1: 508.34   J: 778368.89\n",
            "Iteration 19 :  w1: -276.12   J: 924117.4\n",
            "Iteration 20 :  w1: 578.94   J: 1097281.2\n",
            "Iteration 21 :  w1: -353.08   J: 1303017.12\n",
            "Iteration 22 :  w1: 662.82   J: 1547451.97\n",
            "Iteration 23 :  w1: -444.51   J: 1837865.0\n",
            "Iteration 24 :  w1: 762.48   J: 2182904.73\n"
          ]
        }
      ],
      "source": [
        "# First, complete coding Steps 1.1 through 1.5 in this cell.\n",
        "# Then, run this cell for each of the four scenarios below,\n",
        "# and log your responses for each scenario run in the next cell (Step 1.6)\n",
        "# alpha=0.08; niterations = 25; w1 = 60 # Scenario 1\n",
        "# alpha=0.05; niterations = 25; w1 = 60 # Scenario 2\n",
        "# alpha=0.15; niterations = 25; w1 = 60 # Scenario 3\n",
        "alpha=0.19; niterations = 25; w1 = 60 # Scenario 4\n",
        "\n",
        "# Step 1.1: calculate current value of J at current value of w1\n",
        "# your code here\n",
        "J = calc_J(w1)\n",
        "\n",
        "# Step 1.2: print w1 (rounded to 2 decimal places) and J (rounded to 2 decimal places)\n",
        "# your code here\n",
        "print(round(J,2))\n",
        "\n",
        "# run gradient descent for niterations\n",
        "for i in range(niterations):\n",
        "\n",
        "    # Step 1.3: calculate new value of w1\n",
        "    # your code here\n",
        "    w1 = w1 - alpha * calc_dJ_dw1(w1)\n",
        "\n",
        "    # Step 1.4: calculate new value of J at new w1\n",
        "    # your code here\n",
        "    J = calc_J(w1)\n",
        "\n",
        "    # Step 1.5: print w1 (rounded to 2 decimal places) and J (rounded to 2 decimal places)\n",
        "    # your code here\n",
        "    print(\"Iteration\", i, \":  w1:\", round(w1, 2), \"  J:\", round(J, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8XQpscNbAiO"
      },
      "source": [
        "# Step 1.6: for each Scenario, answer the following questions based on the output (20%)\n",
        "\n",
        "### Scenario 1 (alpha=0.08; niterations = 25; w1 = 60):\n",
        "* did the cost steadily decrease as training progressed?\n",
        "\n",
        "-- Yes, The cost decreased from 3945.06 to 3523 till the 4th interation and then it was constant till the last iteration\n",
        "* did the gradient descent algorithm converge?\n",
        "\n",
        "-- Yes, the algorithm converged and reached the optimal solution by 4th iteration. w1 and the cost decreased till 133 and 3523 resepectively then stayed constant.\n",
        "* what is the minimum lost?\n",
        "\n",
        "-- 3523\n",
        "* what is the optimal value of w1?\n",
        "\n",
        "-- 133\n",
        "* in how many iterations did gradient descent converge (at which row # in the output)?\n",
        "\n",
        "4 iterations\n",
        "\n",
        "### Scenario 2 (alpha=0.05; niterations = 25; w1 = 60):\n",
        "* did the cost steadily decrease as training progressed?\n",
        "\n",
        "-- Yes, It started at 9458.17 and dropped to 3523.0 by iteration 9.\n",
        "After iteration 9, J remained constant at 3523.0, indicating convergence.\n",
        "* did the gradient descent algorithm converge?\n",
        "\n",
        "-- Yes, w1 reached 133.0 and stopped changing after iteration 12. J stopped decreasing after iteration 9, meaning further iterations did not improve the cost.\n",
        "* what is the minimum lost?\n",
        "\n",
        "-- 3523.0\n",
        "* what is the optimal value of w1?\n",
        "\n",
        "-- 133.0\n",
        "* in how many iterations did gradient descent converge (at which row # in the output)?\n",
        "-- 12 iterations\n",
        "\n",
        "### Scenario 3 (alpha=0.15; niterations = 25; w1 = 60):\n",
        "* did the cost steadily decrease as training progressed?\n",
        "\n",
        "-- the cost function J decreased overall but had some fluctuations: From 15906.26 (Iteration 0) to 8754.93 (Iteration 1), Then down to 5733.49 (Iteration 2) Some oscillations occurred between Iterations 2-9, but overall, J kept decreasing. Finally, J stabilized at 3523.0 from Iteration 18 onward. Yes, the cost steadily decreased but had oscillations before convergence.\n",
        "* did the gradient descent algorithm converge?\n",
        "\n",
        "-- J stopped decreasing at Iteration 18 when it reached 3523.0. w1 became stable around 133.0 by Iteration 22. Yes, gradient descent converged at Iteration 18.\n",
        "* what is the minimum lost?\n",
        "\n",
        "-- 3523.0\n",
        "* what is the optimal value of w1?\n",
        "\n",
        "-- 133.0\n",
        "* in how many iterations did gradient descent converge (at which row # in the output)?\n",
        "\n",
        "-- 22 Iterations\n",
        "\n",
        "### Scenario 4 (alpha=0.19; niterations = 25; w1 = 60):\n",
        "* did the cost steadily decrease as training progressed?\n",
        "\n",
        "-- No.\n",
        "The cost function J is increasing instead of decreasing, which indicates that gradient descent is diverging rather than converging. Iteration 0: J = 38,345.62\n",
        "\n",
        "Iteration 5: J=85,960.8 (increased)\n",
        "\n",
        "Iteration 10: J=198,683.25 (keeps increasing)\n",
        "\n",
        "Iteration 20: J=1,097,281.2 (huge increase)\n",
        "\n",
        "Iteration 24: J=2,182,904.73\n",
        "\n",
        "* did the gradient descent algorithm converge?\n",
        "\n",
        "-- No. Instead of stabilizing, J is growing exponentially. w1 is oscillating between positive and negative extreme values. This indicates that the algorithm is diverging instead of converging.\n",
        "\n",
        "* what is the minimum lost?\n",
        "\n",
        "-- 38345.62 in the first iteration only, then it only increased.\n",
        "\n",
        "* what is the optimal value of w1?\n",
        "\n",
        "--There is no optimal value because w1 is oscillating and not converging.\n",
        "\n",
        "* in how many iterations did gradient descent converge (at which row # in the output)?\n",
        "\n",
        "-- It did not converge at all. It is only diverging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16sJ8vw2lFf7"
      },
      "source": [
        "## Exercise 2 (Single Neuron)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgCVy2aUgUMj"
      },
      "source": [
        "### Then, you will apply deep learning thinking in modeling a single neuron to build logistic regression and linear regression models. You need also to Apply the idea of vectorization to efficiently train your models and implement gradient descent from scratch (using multiple iterations of the forward pass, backward propagation, and gradient descent steps) to minimize the loss functions and determine model parameters.\n",
        "\n",
        "### Specifically, there are two parts in this assignment:\n",
        "* In the first part, you will train a logistic regression model using the breast cancer data set. This part has 7 parts (2.1.1 through 2.1.7) that you will need to code (see framework for more details).\n",
        "* In the second part, you will train a linear regression model using the diabetes data set. This part has 7 parts (2.2.1 through 2.2.7) that you will need to code (see framework for more details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM2vgB55gcPu"
      },
      "outputs": [],
      "source": [
        "### DO NOT EDIT THIS CELL!!!\n",
        "### DO NOT IMPORT ANY ADDITIONAL PACKAGES\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG7_IZIVhzGZ"
      },
      "source": [
        "### PART 1\n",
        "\n",
        "* Logistic Regression for Binary Classification using the breast cancer dataset\n",
        "\n",
        "* We aren't going to preprocess data or split into train/test since the focus here is purely on the gradient descent algorithm\n",
        "\n",
        "* NOTE: YOU WILL ONLY EDIT ONE CELL FOR PART-1 (SEE BELOW) (5% for each step, but 25% in total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2P8AGX0PhoCA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X:  (569, 30)\n",
            "y:  (569, 1)\n",
            "m:  569\n",
            "n:  30\n",
            "features:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
          ]
        }
      ],
      "source": [
        "### DO NOT EDIT THIS CELL!!!\n",
        "\n",
        "# read the breast cancer dataset from sklearn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "dataset = load_breast_cancer()\n",
        "\n",
        "# feature matrix X: mxn\n",
        "X = dataset.data\n",
        "print (\"X: \", X.shape)\n",
        "\n",
        "# target vector y: mx1\n",
        "y = dataset.target\n",
        "y = y.reshape(y.shape[0], 1)\n",
        "print (\"y: \", y.shape)\n",
        "\n",
        "# number of samples: m\n",
        "m = X.shape[0]\n",
        "print (\"m: \", m)\n",
        "\n",
        "# number of features: n\n",
        "n = X.shape[1]\n",
        "features = dataset.feature_names\n",
        "print (\"n: \",n)\n",
        "print (\"features: \", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "4H6Zy2Oxh5Gc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9191564147627417\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### THIS IS THE ONLY CELL YOU MUST EDIT FOR PART-1\n",
        "\n",
        "# train the logistic regression model\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "# initializae hyperparameters nepochs and alpha\n",
        "nepochs = 5000\n",
        "alpha = 2.5e-6\n",
        "\n",
        "# initialize model parameters w and b\n",
        "w = np.zeros((n,1))\n",
        "b = 0\n",
        "\n",
        "# this list will collect loss for each iteration\n",
        "costs = []\n",
        "\n",
        "# repeat for nepochs\n",
        "### (2.1.1) YOUR CODE HERE\n",
        "for epoch in range(nepochs):\n",
        "\n",
        "    # forward pass (calculate current loss)\n",
        "    z = np.dot(X, w) + b\n",
        "    yhat = 1 / (1 + np.exp(-z))\n",
        "    J = -1/m * np.sum(y * np.log(yhat) + (1 - y) * np.log(1 - yhat))\n",
        "\n",
        "\n",
        "    # backward propagation (calculate current gradient)\n",
        "    ### (2.1.3) YOUR CODE HERE\n",
        "    dw = 1/m * np.dot(X.T, (yhat - y))\n",
        "    db = 1/m * np.sum(yhat - y)\n",
        "\n",
        "\n",
        "    # gradient descent (update parameters)\n",
        "    ### (2.1.4) YOUR CODE HERE\n",
        "    w = w - alpha * dw\n",
        "    b = b - alpha * db\n",
        "\n",
        "\n",
        "    # append loss to costs: This step is done to keep track of how the loss changes over the course of training\n",
        "    # Costs: It is likely initialized as an empty list before the training loop. It is used to store the loss values at each iteration.\n",
        "    ### (2.1.5) YOUR CODE HERE\n",
        "    costs.append(J)\n",
        "\n",
        "\n",
        "# use final parameters w and b to calculate accuracy for training data X, y\n",
        "#  - do a single forward pass for X: calculate z, yhat\n",
        "#  - set yhat to round(yhat) (that is, thresholding binary classification prediction at 0.5)\n",
        "#  - calculate metrics.accuracy_score(y, yhat) and print it\n",
        "### (2.1.6) YOUR CODE HERE\n",
        "z = np.dot(X, w) + b\n",
        "yhat = 1 / (1 + np.exp(-z))\n",
        "yhat = np.round(yhat)\n",
        "accuracy = metrics.accuracy_score(y, yhat)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# use plt.plot(costs) to plot costs against iterations and show the plot\n",
        "### (2.1.7) YOUR CODE HERE\n",
        "plt.plot(costs)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost vs Iterations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9C3LactGiB2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.92\n"
          ]
        }
      ],
      "source": [
        "### DO NOT EDIT THIS CELL!!!\n",
        "\n",
        "# compare results from your implementation to that of SGDClassifier\n",
        "# your accuracy score above should match (or be very close to) what you get from here\n",
        "\n",
        "nepochs = 5000\n",
        "alpha = 2.5e-6\n",
        "y = y.reshape(y.shape[0],) # to avoid warning\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "clf = SGDClassifier(loss='log_loss', # loss function for logistic regression\n",
        "                    penalty=None, alpha=0, # no regularization\n",
        "                    max_iter=nepochs, tol=None, # stop based only on nepochs\n",
        "                    shuffle=False, random_state=0, # don't shuffle, use random state to replicate results\n",
        "                    learning_rate='constant', eta0=alpha) # constant learning rate of alpha\n",
        "clf.fit(X,y)\n",
        "print (round(clf.score(X,y),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYG7VIaliH4K"
      },
      "source": [
        "### Part 2\n",
        "\n",
        "* Linear Regression using the diabetes dataset\n",
        "\n",
        "* We aren't going to preprocess data or split  into train/test since the focus here is purely on the gradient descent algorithm\n",
        "\n",
        "* NOTE: YOU WILL ONLY EDIT ONE CELL FOR PART-2 (SEE BELOW) (5% for each step, but 25% in total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lli8Djc-Y1Pm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X:  (442, 10)\n",
            "y:  (442, 1)\n",
            "m:  442\n",
            "n:  10\n",
            "features:  ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
          ]
        }
      ],
      "source": [
        "### DO NOT EDIT THIS CELL!!!\n",
        "\n",
        "# read the diabetes dataset from sklearn\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "dataset = load_diabetes()\n",
        "\n",
        "# feature matrix X: mxn\n",
        "X = dataset.data\n",
        "print (\"X: \", X.shape)\n",
        "\n",
        "# target vector y: mx1\n",
        "y = dataset.target\n",
        "y = y.reshape(y.shape[0], 1)\n",
        "print (\"y: \", y.shape)\n",
        "\n",
        "# number of samples: m\n",
        "m = X.shape[0]\n",
        "print (\"m: \", m)\n",
        "\n",
        "# number of features: n\n",
        "n = X.shape[1]\n",
        "features = dataset.feature_names\n",
        "print (\"n: \",n)\n",
        "print (\"features: \", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBPCJeyWiOHC"
      },
      "outputs": [],
      "source": [
        "### THIS IS THE ONLY CELL YOU MUST EDIT FOR PART-2\n",
        "\n",
        "# train the linear regression model\n",
        "\n",
        "\n",
        "# initialize hyperparameters nepochs and alpha\n",
        "nepochs = 5000\n",
        "alpha = 0.5\n",
        "\n",
        "# initialize model paramters w and b\n",
        "w = np.zeros((n,1))\n",
        "b = 0\n",
        "\n",
        "# this list will collect loss for each iteration\n",
        "costs = []\n",
        "\n",
        "# repeat for nepochs\n",
        "for epoch in range(nepochs):\n",
        "\n",
        "    # forward pass (calculate current loss)\n",
        "    z = np.dot(X, w) + b\n",
        "    yhat = z\n",
        "    J = 1/(2*m) * np.sum((yhat - y)**2)\n",
        "\n",
        "    # backward propagation (calculate current gradient)\n",
        "    dw = 1/m * np.dot(X.T, (yhat - y))\n",
        "    db = 1/m * np.sum(yhat - y)\n",
        "\n",
        "    # gradient descent (update parameters)\n",
        "    w = w - alpha * dw\n",
        "    b = b### (2.2.1) YOUR CODE HERE\n",
        "# for ...\n",
        "\n",
        "    # forward pass (calculate current loss)\n",
        "    ### (2.2.2) YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # backward propagation (calculate current gradient)\n",
        "    ### (2.2.3) YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # gradient descent (update parameters)\n",
        "    ### (2.2.4) YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # append loss to costs: This step is done to keep track of how the loss changes over the course of training\n",
        "    # Costs: It is likely initialized as an empty list before the training loop. It is used to store the loss values at each iteration.\n",
        "    ### (2.2.5) YOUR COD HERE\n",
        "\n",
        "\n",
        "# use final parameters w and b to calcualte mse and r2 for training data X, y\n",
        "#  - do a single forward pass for X: calcualte z, yhat\n",
        "#  - calculate metrics.mean_squared_error(y, yhat) and print it\n",
        "#  - calculate metrics.r2_score(y, yhat) and print it\n",
        "### (2.2.6) YOUR CODE HERE\n",
        "\n",
        "\n",
        "# use plt.plot(costs) to plot costs against iterations and show the plot\n",
        "### (2.2.7) YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0fI3RBEiTGx"
      },
      "outputs": [],
      "source": [
        "### DO NOT EDIT THIS CELL!!!\n",
        "\n",
        "# compare results from our implementation to that of SGDRegressor\n",
        "# your mse and r2 score above should match (or be very close to) what you get from here\n",
        "\n",
        "nepochs = 5000\n",
        "alpha = 2.5e-6\n",
        "y = y.reshape(y.shape[0],) # to avoid warning\n",
        "\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "clf = SGDRegressor(loss='squared_error', # mse for linear regression\n",
        "                    penalty=None, alpha=0, # no regularization\n",
        "                    max_iter=nepochs, tol=None, # stop based on only on nepochs\n",
        "                    shuffle=False, random_state=0, # don't shuffle, use random state to replicate results\n",
        "                    learning_rate='constant', eta0=alpha) # constant learning rate of alpha\n",
        "clf.fit(X,y)\n",
        "yhat = clf.predict(X)\n",
        "print (round(metrics.mean_squared_error(y, yhat),2))\n",
        "print (round(clf.score(X,y),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxneXnBSqLgk"
      },
      "source": [
        "## Acknowledgment\n",
        "\n",
        "Acknowledge here if you have used any GenAI tools in this assignment and anyone you have worked together with on this assignment. (5%)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
