{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ Lab Assignment 4\n",
    "\n",
    "~ Rhythm Rajesh Shah\n",
    "\n",
    "~ 1233960561\n",
    "\n",
    "~ 02/22/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 1 (5%) - Import all the necessary libraries and restaurant review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from keras.layers import TextVectorization\n",
    "from keras.utils import pad_sequences\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IVS7do_HBzroiCiymNdxDg</td>\n",
       "      <td>fdFgZQQYQJeEAshH4lxSfQ</td>\n",
       "      <td>sGy67CpJctjeCWClWqonjA</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>OK, the hype about having Hatch chili in your ...</td>\n",
       "      <td>2020-01-27 22:59:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QP2pSzSqpJTMWOCuUuyXkQ</td>\n",
       "      <td>JBLWSXBTKFvJYYiM-FnCOQ</td>\n",
       "      <td>3w7NRntdQ9h0KwDsksIt5Q</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pandemic pit stop to have an ice cream.... onl...</td>\n",
       "      <td>2020-04-19 05:33:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oK0cGYStgDOusZKz9B1qug</td>\n",
       "      <td>2_9fKnXChUjC5xArfF8BLg</td>\n",
       "      <td>OMnPtRGmbY8qH_wIILfYKA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I was lucky enough to go to the soft opening a...</td>\n",
       "      <td>2020-02-29 19:43:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_ABvFCNVLbfOgRg3Pv1KQ</td>\n",
       "      <td>9MExTQ76GSKhxSWnTS901g</td>\n",
       "      <td>V9XlikTxq0My4gE8LULsjw</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I've gone to claim Jumpers all over the US and...</td>\n",
       "      <td>2020-03-14 21:47:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rd222CrrnXkXukR2iWj69g</td>\n",
       "      <td>LPxuausjvDN88uPr-Q4cQA</td>\n",
       "      <td>CA5BOxKRDPGJgdUQ8OUOpw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you haven't been  to Maynard's kitchen, it'...</td>\n",
       "      <td>2020-01-17 20:32:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  IVS7do_HBzroiCiymNdxDg  fdFgZQQYQJeEAshH4lxSfQ  sGy67CpJctjeCWClWqonjA   \n",
       "1  QP2pSzSqpJTMWOCuUuyXkQ  JBLWSXBTKFvJYYiM-FnCOQ  3w7NRntdQ9h0KwDsksIt5Q   \n",
       "2  oK0cGYStgDOusZKz9B1qug  2_9fKnXChUjC5xArfF8BLg  OMnPtRGmbY8qH_wIILfYKA   \n",
       "3  E_ABvFCNVLbfOgRg3Pv1KQ  9MExTQ76GSKhxSWnTS901g  V9XlikTxq0My4gE8LULsjw   \n",
       "4  Rd222CrrnXkXukR2iWj69g  LPxuausjvDN88uPr-Q4cQA  CA5BOxKRDPGJgdUQ8OUOpw   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      3       1      1     0   \n",
       "1      5       1      1     1   \n",
       "2      5       1      0     0   \n",
       "3      5       0      0     0   \n",
       "4      4       1      0     0   \n",
       "\n",
       "                                                text                 date  \n",
       "0  OK, the hype about having Hatch chili in your ...  2020-01-27 22:59:06  \n",
       "1  Pandemic pit stop to have an ice cream.... onl...  2020-04-19 05:33:16  \n",
       "2  I was lucky enough to go to the soft opening a...  2020-02-29 19:43:44  \n",
       "3  I've gone to claim Jumpers all over the US and...  2020-03-14 21:47:07  \n",
       "4  If you haven't been  to Maynard's kitchen, it'...  2020-01-17 20:32:57  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"restaurant_reviews_az.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 2 (5%) -  Remove 3-star reviews from the input data and create a new column - Sentiment for the remaining reviews. For reviews with 1 or 2 star ratings, set the value in the Sentiment column to 0. For reviews with 4 or 5-star ratings, set the value in the sentiment column to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QP2pSzSqpJTMWOCuUuyXkQ</td>\n",
       "      <td>JBLWSXBTKFvJYYiM-FnCOQ</td>\n",
       "      <td>3w7NRntdQ9h0KwDsksIt5Q</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pandemic pit stop to have an ice cream.... onl...</td>\n",
       "      <td>2020-04-19 05:33:16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oK0cGYStgDOusZKz9B1qug</td>\n",
       "      <td>2_9fKnXChUjC5xArfF8BLg</td>\n",
       "      <td>OMnPtRGmbY8qH_wIILfYKA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I was lucky enough to go to the soft opening a...</td>\n",
       "      <td>2020-02-29 19:43:44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_ABvFCNVLbfOgRg3Pv1KQ</td>\n",
       "      <td>9MExTQ76GSKhxSWnTS901g</td>\n",
       "      <td>V9XlikTxq0My4gE8LULsjw</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I've gone to claim Jumpers all over the US and...</td>\n",
       "      <td>2020-03-14 21:47:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rd222CrrnXkXukR2iWj69g</td>\n",
       "      <td>LPxuausjvDN88uPr-Q4cQA</td>\n",
       "      <td>CA5BOxKRDPGJgdUQ8OUOpw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you haven't been  to Maynard's kitchen, it'...</td>\n",
       "      <td>2020-01-17 20:32:57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kx6O_lyLzUnA7Xip5wh2NA</td>\n",
       "      <td>YsINprB2G1DM8qG1hbrPUg</td>\n",
       "      <td>rViAhfKLKmwbhTKROM9m0w</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I stay at the Main Hotel at the Casino from Ju...</td>\n",
       "      <td>2020-07-14 16:43:23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "1  QP2pSzSqpJTMWOCuUuyXkQ  JBLWSXBTKFvJYYiM-FnCOQ  3w7NRntdQ9h0KwDsksIt5Q   \n",
       "2  oK0cGYStgDOusZKz9B1qug  2_9fKnXChUjC5xArfF8BLg  OMnPtRGmbY8qH_wIILfYKA   \n",
       "3  E_ABvFCNVLbfOgRg3Pv1KQ  9MExTQ76GSKhxSWnTS901g  V9XlikTxq0My4gE8LULsjw   \n",
       "4  Rd222CrrnXkXukR2iWj69g  LPxuausjvDN88uPr-Q4cQA  CA5BOxKRDPGJgdUQ8OUOpw   \n",
       "5  kx6O_lyLzUnA7Xip5wh2NA  YsINprB2G1DM8qG1hbrPUg  rViAhfKLKmwbhTKROM9m0w   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "1      5       1      1     1   \n",
       "2      5       1      0     0   \n",
       "3      5       0      0     0   \n",
       "4      4       1      0     0   \n",
       "5      1       0      0     0   \n",
       "\n",
       "                                                text                 date  \\\n",
       "1  Pandemic pit stop to have an ice cream.... onl...  2020-04-19 05:33:16   \n",
       "2  I was lucky enough to go to the soft opening a...  2020-02-29 19:43:44   \n",
       "3  I've gone to claim Jumpers all over the US and...  2020-03-14 21:47:07   \n",
       "4  If you haven't been  to Maynard's kitchen, it'...  2020-01-17 20:32:57   \n",
       "5  I stay at the Main Hotel at the Casino from Ju...  2020-07-14 16:43:23   \n",
       "\n",
       "   Sentiment  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "5          0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['stars'] != 3]\n",
    "\n",
    "df['Sentiment'] = df['stars'].apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 3 (10%) - Conduct necessary data processing. Prepare the training and test sets on review data for machine learning classifications. 20% of the data is for testing, and 80% of the data is for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_single_review(doc, lowercase=True, remove_stopwords=True):\n",
    "    lemmatized = []\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_.lower() if lowercase else token.lemma_\n",
    "        if not remove_stopwords or (remove_stopwords and not token.is_stop):\n",
    "            lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "# Process the dataset in chunks\n",
    "def process_in_chunks(df, chunk_size=1000, batch_size=500, lowercase=True, remove_stopwords=True):\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(df), chunk_size)):  # Loop through the dataset in chunks\n",
    "        chunk = df['text'].iloc[i:i+chunk_size].tolist()\n",
    "\n",
    "        # Process each chunk using spaCy's pipe for efficiency\n",
    "        processed_chunk = []\n",
    "        for doc in nlp.pipe(chunk, batch_size=batch_size, n_process=1):  # Use a limited number of processes\n",
    "            processed_chunk.append(normalize_single_review(doc, lowercase=lowercase, remove_stopwords=remove_stopwords))\n",
    "\n",
    "        results.extend(processed_chunk)  # Add the processed chunk to results\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#methods from nltk\n",
    "# to remove non alphanumeric character\n",
    "def alpha_num(text):\n",
    "    return re.sub(r'[^A-Za-z0-9 ]', '', text)\n",
    "\n",
    "# to remove URLs\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "# to remove html tags\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_URL function done\n",
      "remove_html function done\n",
      "alpha_num function done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [06:34<00:00,  8.77s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QP2pSzSqpJTMWOCuUuyXkQ</td>\n",
       "      <td>JBLWSXBTKFvJYYiM-FnCOQ</td>\n",
       "      <td>3w7NRntdQ9h0KwDsksIt5Q</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pandemic pit stop to have an ice cream only pl...</td>\n",
       "      <td>2020-04-19 05:33:16</td>\n",
       "      <td>1</td>\n",
       "      <td>pandemic pit stop ice cream plain sundae limit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oK0cGYStgDOusZKz9B1qug</td>\n",
       "      <td>2_9fKnXChUjC5xArfF8BLg</td>\n",
       "      <td>OMnPtRGmbY8qH_wIILfYKA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I was lucky enough to go to the soft opening a...</td>\n",
       "      <td>2020-02-29 19:43:44</td>\n",
       "      <td>1</td>\n",
       "      <td>lucky soft opening let tell good beer wine dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_ABvFCNVLbfOgRg3Pv1KQ</td>\n",
       "      <td>9MExTQ76GSKhxSWnTS901g</td>\n",
       "      <td>V9XlikTxq0My4gE8LULsjw</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ive gone to claim Jumpers all over the US and ...</td>\n",
       "      <td>2020-03-14 21:47:07</td>\n",
       "      <td>1</td>\n",
       "      <td>ve go claim jumper disappoint location differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rd222CrrnXkXukR2iWj69g</td>\n",
       "      <td>LPxuausjvDN88uPr-Q4cQA</td>\n",
       "      <td>CA5BOxKRDPGJgdUQ8OUOpw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you havent been  to Maynards kitchen its ti...</td>\n",
       "      <td>2020-01-17 20:32:57</td>\n",
       "      <td>1</td>\n",
       "      <td>not   maynards kitchen time hope dinner sure r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kx6O_lyLzUnA7Xip5wh2NA</td>\n",
       "      <td>YsINprB2G1DM8qG1hbrPUg</td>\n",
       "      <td>rViAhfKLKmwbhTKROM9m0w</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I stay at the Main Hotel at the Casino from Ju...</td>\n",
       "      <td>2020-07-14 16:43:23</td>\n",
       "      <td>0</td>\n",
       "      <td>stay main hotel casino july 11 july 13 bad exp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "1  QP2pSzSqpJTMWOCuUuyXkQ  JBLWSXBTKFvJYYiM-FnCOQ  3w7NRntdQ9h0KwDsksIt5Q   \n",
       "2  oK0cGYStgDOusZKz9B1qug  2_9fKnXChUjC5xArfF8BLg  OMnPtRGmbY8qH_wIILfYKA   \n",
       "3  E_ABvFCNVLbfOgRg3Pv1KQ  9MExTQ76GSKhxSWnTS901g  V9XlikTxq0My4gE8LULsjw   \n",
       "4  Rd222CrrnXkXukR2iWj69g  LPxuausjvDN88uPr-Q4cQA  CA5BOxKRDPGJgdUQ8OUOpw   \n",
       "5  kx6O_lyLzUnA7Xip5wh2NA  YsINprB2G1DM8qG1hbrPUg  rViAhfKLKmwbhTKROM9m0w   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "1      5       1      1     1   \n",
       "2      5       1      0     0   \n",
       "3      5       0      0     0   \n",
       "4      4       1      0     0   \n",
       "5      1       0      0     0   \n",
       "\n",
       "                                                text                 date  \\\n",
       "1  Pandemic pit stop to have an ice cream only pl...  2020-04-19 05:33:16   \n",
       "2  I was lucky enough to go to the soft opening a...  2020-02-29 19:43:44   \n",
       "3  Ive gone to claim Jumpers all over the US and ...  2020-03-14 21:47:07   \n",
       "4  If you havent been  to Maynards kitchen its ti...  2020-01-17 20:32:57   \n",
       "5  I stay at the Main Hotel at the Casino from Ju...  2020-07-14 16:43:23   \n",
       "\n",
       "   Sentiment                                            preview  \n",
       "1          1  pandemic pit stop ice cream plain sundae limit...  \n",
       "2          1  lucky soft opening let tell good beer wine dif...  \n",
       "3          1  ve go claim jumper disappoint location differe...  \n",
       "4          1  not   maynards kitchen time hope dinner sure r...  \n",
       "5          0  stay main hotel casino july 11 july 13 bad exp...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply preprocessing steps\n",
    "df['text'] = df['text'].apply(remove_URL)\n",
    "print(\"remove_URL function done\")\n",
    "df['text'] = df['text'].apply(remove_html)\n",
    "print(\"remove_html function done\")\n",
    "df['text'] = df['text'].apply(alpha_num)\n",
    "print(\"alpha_num function done\")\n",
    "# Apply the chunked processing to the entire dataframe\n",
    "# df['review'] = df['review'].apply(normalize,lowercase=True, remove_stopwords=True)\n",
    "df['preview'] = process_in_chunks(df, chunk_size=1000, batch_size=500, lowercase=True, remove_stopwords=True)\n",
    "\n",
    "# Display the processed dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(df['preview'], df['Sentiment'], test_size=0.2, random_state=5, stratify=df['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 4 (10%) - Download the pre-trained GloVe word embeddings (glove.6B.100d.txt) and prepare the embedding matrix for this review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
